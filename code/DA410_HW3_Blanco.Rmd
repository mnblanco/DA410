---
title: 'Chapter 6: Multivariate Analysis of Variance'
author: "Marjorie Blanco"
subtitle: DA 410
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
library(readr)
library(kableExtra)
library(gplots)
library(matlib)
library(stats)
library(sjPlot)
library(sjmisc)
library(ggplot2)
library(dplyr)
theme_set(theme_sjplot())
```

## Problem 6.27

Baten, Tack, and Baeder (1958) compared judges' scores on fish prepared by three methods. Twelve fish were cooked by each method, and several judges tasted fish samples and rated each on four variables: $y_1$ = aroma, $y_2$ = flavor, $y_3$ = texture, and $y_4$ = moisture. The data are in Table 6.17. Each entry is an average score for the judges on that fish.

```{r echo=FALSE}
fish <- read_table2("Software-Files/T6_17_FISH.CSV", 
                    col_names = c("Method", "y1", "y2", "y3", "y4", "y5"), 
                    col_types = cols(y5 = col_skip()))
fish$Method <- as.factor(fish$Method)
```

```{r echo=FALSE}
kable(fish) %>%
  kable_styling(bootstrap_options = "striped")
```

```{r}
plotmeans(y1 ~ Method, data = fish,
          xlab = "Method", ylab = "aroma",
          main="Mean Plot with 95% CI") 

plotmeans(y2 ~ Method, data = fish,
          xlab = "Method", ylab = "flavor",
          main="Mean Plot with 95% CI") 

plotmeans(y3 ~ Method, data = fish,
          xlab = "Method", ylab = "texture",
          main="Mean Plot with 95% CI") 

plotmeans(y4 ~ Method, data = fish,
          xlab = "Method", ylab = "moisture",
          main="Mean Plot with 95% CI") 
```

(a) Compare the three methods using all four MANOVA tests.

```{r echo=FALSE}
n <- dim(fish)[1] / length(unique(fish$Method))
total.means <- colMeans(fish[,2:5])
```

The overall mean vector:

```{r echo=FALSE}
kable(total.means) %>%
  kable_styling(bootstrap_options = "striped")
```

```{r echo=FALSE}
fish.group <- split(fish[,2:5], fish$Method)
fish.means <- sapply(fish.group, function(x) {
  apply(x, 2, mean)
}, simplify = 'data.frame')
```

The mean vectors represent 3 points in four-dementional space.  The three mean vectors:

```{r echo=FALSE}
kable(fish.means) %>%
  kable_styling(bootstrap_options = "striped")
```

```{r echo=FALSE}
H <- matrix(data = 0, nrow = 4, ncol = 4)
for (i in 1:dim(H)[1]) {
  for (j in 1:i) {
    H[i,j] <- n * sum((fish.means[i,] - total.means[i]) * (fish.means[j,] - total.means[j]))
    H[j,i] <- n * sum((fish.means[j,] - total.means[j]) * (fish.means[i,] - total.means[i]))
  }
}
```

H = 
```{r echo=FALSE}
kable(H) %>% 
  kable_styling(bootstrap_options = "striped")
```

```{r echo=FALSE}
E <- matrix(data = 0, nrow = 4, ncol = 4)
for (i in 1:dim(E)[1]) {
  for (j in 1:i) {
    b <- c() 
    for (k in fish.group) {
      k <- as.matrix(k)
      a <- sum((k[,i] - mean(k[,i])) * (k[,j] - mean(k[,j])))
      b <- append(b, a)
    }
    E[i,j] <- sum(b)
    E[j,i] <- sum(b)
  }
}
```

E = 
```{r echo=FALSE}
kable(E) %>% 
  kable_styling(bootstrap_options = "striped")
```

```{r echo=FALSE}
k <- length(unique(fish$Method))
p <- length(fish[,2:5])
vh <- k - 1
ve <- dim(fish)[1] - k
t <- sqrt((p^2 * vh^2 - 4) / (p^2 + vh^2 -5))
df1 <- p * vh
df2 <- (ve + vh - .5 * (p + vh + 1)) * t - .5 * (p * vh - 2)
f <- (1 - (det(E) / det(E + H))^(1/t)) / (det(E) / det(E + H))^(1/t) * df2 / df1
```

k = `r k`

p = `r p`

$_vH$ = `r vh`

$_vE$ = `r ve`

t = `r t`

$df_1$ = `r df1`

$df_2$  = `r df2`

F = `r f`

```{r}
# MANOVA test
fish.manova <- manova(cbind(fish$y1, fish$y2, fish$y3, fish$y4) ~ Method, data = fish)
fish.summary <- summary(fish.manova)
fish.summary

# Look to see which differ
summary.aov(fish.manova)
```

```{r}
fish.summary$SS
```

We would then like to test if the properties are the same across the three cooking methods.

$H_0: \mu_1 = \mu_2 = \mu_3$

$H_1:$ The $\mu's$ are unequal

```{r echo=FALSE}
# Wilk’s Lambda 
lambda <- det(E) / det(E + H)
e1h.eigen <- eigen(solve(E) %*% H)
Vs <- sum(e1h.eigen$values / (1 + e1h.eigen$values))
Us <- sum(e1h.eigen$values)
s <- min(vh, p)
roy.stat <- e1h.eigen$values[1]
roy.omega <- roy.stat / (1 + roy.stat)
```

```{r}
summary(manova(cbind(fish$y1, fish$y2, 
                     fish$y3, fish$y4) ~ fish$Method), test = "Wilks")
```

$\lambda$ =  `r round(lambda, 3)`

The Wilks’s test rejects the hypothesis $H_0$ that the mean vector for the three cooking methods are equal.

```{r}
summary(manova(cbind(fish$y1, fish$y2, 
                     fish$y3, fish$y4) ~ fish$Method), test = "Roy")
```

$\Theta$ =  `r round(roy.omega, 3)`

The Roy’s test also rejects the hypothesis $H_0$ that the mean vector for the three cooking methods are equal.

```{r}
summary(manova(cbind(fish$y1, fish$y2, 
                     fish$y3, fish$y4) ~ fish$Method), test = "Hotelling-Lawley")
```

$U^{(s)}$ = `r round(Us, 3)`

The Hotelling-Lawley’s test also rejects the hypothesis $H_0$ that the mean vector for the three cooking methods are equal.

```{r}
summary(manova(cbind(fish$y1, fish$y2, 
                     fish$y3, fish$y4) ~ fish$Method), test = "Pillai")
```

$V^{(s)}$ = `r round(Vs,  3)`

The Pillai’s test also rejects the hypothesis $H_0$ that the mean vector for the three cooking methods are equal.

Answer using the tip sheet

```{r}
method1 <- fish %>% filter(Method == 1)  %>% select(-Method)
method2 <- fish %>% filter(Method == 2)  %>% select(-Method)
method3 <- fish %>% filter(Method == 3)  %>% select(-Method)
  
method1.bar <- colMeans(method1)
method2.bar <- colMeans(method2) 
method3.bar <- colMeans(method3) 
method.all.bar <- (method1.bar+method2.bar+method3.bar)/3

method1.bar.diff <- method1.bar - method.all.bar 
method2.bar.diff <- method2.bar - method.all.bar 
method3.bar.diff <- method3.bar - method.all.bar 

H <- 12 * unname(method1.bar.diff %*% t(method1.bar.diff) +
                   method2.bar.diff %*% t(method2.bar.diff) +
                   method3.bar.diff %*% t(method3.bar.diff))
```

H =
```{r echo=FALSE}
kable(H) %>% 
  kable_styling(bootstrap_options = "striped")
```

```{r}
"compute.within.matrix" <-function(data, mean) {
  ret <- matrix(as.numeric(0), nrow=4, ncol=4)
  for (i in 1:12) {
    diff <- as.numeric(unname(data[i,] - mean)) 
    ret <- ret + diff %*% t(diff)} 
  return(ret)
  }
E <- compute.within.matrix(method1, method1.bar) + compute.within.matrix(method2, method2.bar) + compute.within.matrix(method3, method3.bar)
```

E =
```{r echo=FALSE}
kable(E) %>% 
  kable_styling(bootstrap_options = "striped")
```

```{r}
Lambda <-det(E) / det(E + H)
V.s <- tr(solve(E + H) %*% H)
U.s <-tr(solve(E) %*% H)
lambda.1 <-eigen(solve(E) %*% H)$values[1]
theta <- lambda.1 / (1 + lambda.1)
```

$\lambda$ =  `r round(Lambda, 3)`

The Wilks’s test rejects the hypothesis $H_0$ that the mean vector for the three cooking methods are equal.

$\Theta$ =  `r round(theta, 3)`

The Roy’s test also rejects the hypothesis $H_0$ that the mean vector for the three cooking methods are equal.

$U^{(s)}$ = `r round(U.s, 3)`

The Hotelling-Lawley’s test also rejects the hypothesis $H_0$ that the mean vector for the three cooking methods are equal.

$V^{(s)}$ = `r round(V.s, 3)`

The Pillai’s test also rejects the hypothesis $H_0$ that the mean vector for the three cooking methods are equal.



(b) Compute the following measures of multivariate association from Section 6.1.8 : $\eta^2_\Lambda$, $\eta^2_\Theta$, $A_\Lambda$, $A_{LH}$, $A_P$.

$\eta^2_\Lambda$ = `r round(1 - lambda, 3)`

$\eta^2_\Theta$ = $\Theta$ = `r round(roy.omega, 3)`

$A_\Lambda$ = `r round(1 - lambda^(1/s), 3)`

$A_{LH}$ = `r round((Us/s) / (1+Us /s), 3)`

$A_P$ = `r round(Vs/s, 3)`

(c) Based on the eigenvalues, is the essential dimensionality of the space containing the mean vectors equal to 1 or 2?

 The eigenvalues of $E^{-1}$  H are

```{r}
eigen(inv(E) %*% H)
```

The essential dimensionality of the space containing the mean vectors is equal to 1.

## Problem 6.28

Table 6.18, from Keuls, Martakis, and Magid (1984), gives data from a two-way (fixed-effects) MANOVA on snap beans showing the results of four variables: $y_1$ = yield earliness, $y_2$ = specific leaf area (SLA) earliness, $y_3$ = total yield, and $y_4$ = average SLA. The factors are sowing date (S) and variety (V).

```{r echo=FALSE, warning=FALSE, message=FALSE}
snapbean <- read_table2("Software-Files/T6_18_SNAPBEAN.CSV", 
                    col_names = c("S", "V", "record",  "y1", "y2", "y3", "y4"))

snapbean$S <- as.factor(snapbean$S)
snapbean$V <- as.factor(snapbean$V)

fit1 <- lm(y1 ~ S * V, data = snapbean)
fit2 <- lm(y2 ~ S * V, data = snapbean)
fit3 <- lm(y3 ~ S * V, data = snapbean)
fit4 <- lm(y4 ~ S * V, data = snapbean)
```

```{r echo=FALSE}
kable(head(snapbean, 10)) %>%
  kable_styling(bootstrap_options = "striped")
```

(a) Test for main effects and interaction using all four MANOVA statistics.


```{r}
snapbean.manova <- manova(cbind(snapbean$y1, snapbean$y2, 
                                snapbean$y3, snapbean$y4) ~ snapbean$S * snapbean$V, 
                          data = snapbean)
snapbean.summary <- summary(snapbean.manova)
snapbean.summary
```


```{r}
plot_model(fit1, type = "pred", terms = c("S", "V") , 
           legend.title = "Variety", axis.title = c("Sowing date", "Yield earliness"), title = "")
plot_model(fit2, type = "pred", terms = c("S", "V") , 
           legend.title = "Variety", axis.title = c("Sowing date","SLA"), title = "")
plot_model(fit3, type = "pred", terms = c("S", "V") , 
           legend.title = "Variety", axis.title = c("Sowing date","Total yield"), title = "")
plot_model(fit4, type = "pred", terms = c("S", "V") , 
           legend.title = "Variety", axis.title = c("Sowing date", "Average SLA"), title = "")
```

### Error

$E$ =
```{r echo=FALSE}
E <- snapbean.summary$SS$Residuals
kable(E) %>%
  kable_styling(bootstrap_options = "striped")
```

### S effect

$H_S$ =
```{r echo=FALSE}
HS <- snapbean.summary$SS$`snapbean$S`
kable(HS) %>%
  kable_styling(bootstrap_options = "striped")

lambdaS <- det(E) / det(E + HS)
e1h.eigen <- eigen(solve(E) %*% HS)
Vs <- sum(e1h.eigen$values / (1 + e1h.eigen$values))
Us <- sum(e1h.eigen$values)
roy.omega <- e1h.eigen$values[1] / (1 + e1h.eigen$values[1])
```

$V^{(s)}$ = `r Vs`
$U^{(s)}$ = `r Us`
$\Theta$ = `r roy.omega`

The main effect and interaction are statistically significant.

### V effect

$H_V$ =
```{r echo=FALSE}
HV <- snapbean.summary$SS$`snapbean$V`
kable(HV) %>%
  kable_styling(bootstrap_options = "striped")
lambdaV <- det(E) / det(E + HV)
e1h.eigen <- eigen(solve(E) %*% HV)
Vs <- sum(e1h.eigen$values / (1 + e1h.eigen$values))
Us <- sum(e1h.eigen$values)
roy.omega <- e1h.eigen$values[1] / (1 + e1h.eigen$values[1])
```

$\Lambda$ = `r lambdaV`
$V^{(s)}$ = `r Vs`
$U^{(s)}$ = `r Us`
$\Theta$ = `r roy.omega`

The main effect and interaction are statistically significant.

### SV effect

$H_{SV}$ =
```{r echo=FALSE}
HSV <- snapbean.summary$SS$`snapbean$S:snapbean$V`
kable(HSV) %>%
  kable_styling(bootstrap_options = "striped")
lambdaSV <- det(E) / det(E + HSV)
e1h.eigen <- eigen(solve(E) %*% HSV)
Vs <- sum(e1h.eigen$values / (1 + e1h.eigen$values))
Us <- sum(e1h.eigen$values)
roy.omega <- e1h.eigen$values[1] / (1 + e1h.eigen$values[1])
```

$\Lambda$ = `r lambdaSV`
$V^{(s)}$ = `r Vs`
$U^{(s)}$ = `r Us`
$\Theta$ = `r roy.omega`

The main effect and interaction are statistically significant.

```{r}
summary(manova(snapbean.manova), test = "Wilks")
```

The main effect and interaction are statistically significant.

```{r}
summary(manova(snapbean.manova), test = "Pillai")
```

The main effect and interaction are statistically significant.

```{r}
summary(manova(snapbean.manova), test = "Hotelling-Lawley")
```

The main effect and interaction are statistically significant.

```{r}
summary(manova(snapbean.manova), test = "Roy")
```

The main effect and interaction are statistically significant.

